# -*- coding: utf-8 -*-
"""onion_CoatNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xCyBjOGC7W-5bqvmJcfB84txOykXApeg
"""

# STEP 1: Upload your service account JSON key
from google.colab import files
uploaded = files.upload()  # Upload the JSON file (e.g., onion11-key.json)

# STEP 2: Move the uploaded file and authenticate GCP
import os

json_filename = list(uploaded.keys())[0]  # Get uploaded JSON filename
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = json_filename
print(f"Using service account: {json_filename}")

# STEP 3: Download dataset from GCS using gsutil
GCS_URI = "gs://onion11"
!mkdir -p OnionData
!gsutil -m cp -r {GCS_URI}/* OnionData/

# STEP 4: Set up data directories and split into train/val/test
import os
import shutil
import random
import warnings
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Base folder containing all .jpg and .txt files
base_dir = 'OnionData'

# Optional: parse class index to name mapping from classes.txt
class_id_to_name = {}
classes_txt_path = os.path.join(base_dir, 'classes.txt')
if os.path.exists(classes_txt_path):
    with open(classes_txt_path, 'r') as f:
        for i, line in enumerate(f):
            class_id_to_name[str(i)] = line.strip()

# Get all base names with both .jpg and .txt present
jpgs = {os.path.splitext(f)[0]: f for f in os.listdir(base_dir) if f.endswith('.jpg')}
txts = {os.path.splitext(f)[0]: f for f in os.listdir(base_dir) if f.endswith('.txt')}

paired = [base for base in jpgs if base in txts]

print(f"Found {len(paired)} valid image-label pairs.")

# Group images by class
class_to_images = {}

for base in paired:
    txt_path = os.path.join(base_dir, txts[base])
    try:
        with open(txt_path, 'r') as f:
            line = f.readline().strip()
            if not line:
                continue
            class_id = line.split()[0]  # First token is class index
            class_label = class_id_to_name.get(class_id, f"class_{class_id}")  # Optional mapping
            if class_label not in class_to_images:
                class_to_images[class_label] = []
            class_to_images[class_label].append(jpgs[base])
    except Exception as e:
        print(f"Skipping {txt_path} due to error: {e}")

# Prepare train, val, test directories
for split in ['train', 'val', 'test']:
    shutil.rmtree(split, ignore_errors=True)
    for label in class_to_images.keys():
        os.makedirs(os.path.join(split, label), exist_ok=True)

# Split and copy images
for label, images in class_to_images.items():
    random.shuffle(images)
    total = len(images)
    train_n = int(0.8 * total)
    val_n = int(0.1 * total)
    test_n = total - train_n - val_n

    for img in images[:train_n]:
        shutil.copyfile(os.path.join(base_dir, img), os.path.join('train', label, img))
    for img in images[train_n:train_n+val_n]:
        shutil.copyfile(os.path.join(base_dir, img), os.path.join('val', label, img))
    for img in images[train_n+val_n:]:
        shutil.copyfile(os.path.join(base_dir, img), os.path.join('test', label, img))

# Show final stats
for split in ['train', 'val', 'test']:
    print(f"\n{split.upper()}")
    for label in sorted(class_to_images.keys()):
        count = len(os.listdir(os.path.join(split, label)))
        print(f"  {label}: {count} images")

# STEP 5: Data loading and augmentation
IMG_SIZE = 224
BATCH_SIZE = 32

train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
    'train', target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')

val_generator = val_datagen.flow_from_directory(
    'val', target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')

test_generator = test_datagen.flow_from_directory(
    'test', target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')

print("Detected classes:", train_generator.class_indices)

# STEP 6: Define CoatNet-like model
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, MaxPooling2D, GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.optimizers import Adam

def coatnet_block(x, filters):
    x = Conv2D(filters, (3, 3), padding="same", activation="relu")(x)
    x = BatchNormalization()(x)
    x = Conv2D(filters, (3, 3), padding="same", activation="relu")(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    return x

def build_coatnet(input_shape, num_classes):
    inputs = Input(shape=input_shape)
    x = coatnet_block(inputs, 64)
    x = coatnet_block(x, 128)
    x = coatnet_block(x, 256)
    x = GlobalAveragePooling2D()(x)
    x = Dense(512, activation="relu")(x)
    x = Dropout(0.5)(x)
    x = Dense(num_classes, activation="softmax")(x)
    return Model(inputs, x)

input_shape = (IMG_SIZE, IMG_SIZE, 3)
num_classes = len(train_generator.class_indices)

model = build_coatnet(input_shape, num_classes)
model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# STEP 7: Train model
import time
EPOCHS = 10

start_time = time.time()
history = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS)
training_time = time.time() - start_time

import matplotlib.pyplot as plt

test_loss, test_accuracy = model.evaluate(test_generator)

# Accuracy curves
plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Accuracy over Epochs")
plt.grid(True)
plt.show()

# STEP 10: Classification report, confusion matrix, and ROC AUC
import numpy as np

# Model size
model_size = sum([np.prod(v.shape) for v in model.trainable_variables])
model_size_MB = model_size * 4 / (1024 ** 2)  # 4 bytes per float32

print(f"\n‚úÖ Training Accuracy: {history.history['accuracy'][-1] * 100:.2f}%")
print(f"‚úÖ Validation Accuracy: {history.history['val_accuracy'][-1] * 100:.2f}%")
print(f"‚úÖ Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"‚è±Ô∏è Training Time: {training_time:.2f} seconds")
print(f"üì¶ Model Size: {model_size_MB:.2f} MB")

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import seaborn as sns

# Predictions
y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = test_generator.classes
class_labels = list(test_generator.class_indices.keys())

# Classification report
print("\nüìä Classification Report:")
print(classification_report(y_true, y_pred_classes, target_names=class_labels))

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

# AUC-ROC
y_true_bin = label_binarize(y_true, classes=range(len(class_labels)))

plt.figure(figsize=(12, 8))
for i, label in enumerate(class_labels):
    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f"{label} (AUC = {roc_auc:.2f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("AUC-ROC Curve (One-vs-Rest)")
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# Macro & weighted AUC
print(f"üßÆ Macro-average AUC: {roc_auc_score(y_true_bin, y_pred, average='macro'):.4f}")
print(f"üßÆ Weighted-average AUC: {roc_auc_score(y_true_bin, y_pred, average='weighted'):.4f}")
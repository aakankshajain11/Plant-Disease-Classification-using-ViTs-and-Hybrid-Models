# -*- coding: utf-8 -*-
"""CoatNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wD9CvRCtxjwVoI9CTRUhbDue1CJ8n8jn
"""

from google.colab import files
files.upload()  # Upload kaggle.json file

import os
import zipfile

# Move kaggle.json to the correct location
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json  # Set permission

# Download the dataset
!kaggle datasets download -d emmarex/plantdisease

# Unzip the dataset
with zipfile.ZipFile("plantdisease.zip", "r") as zip_ref:
    zip_ref.extractall("PlantVillage")

# Check extracted files
os.listdir("PlantVillage")

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import time
import os
import shutil
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout, Input
from tensorflow.keras.optimizers import Adam
import psutil

# Set paths
dataset_path = "PlantVillage/PlantVillage"
train_dir = "train"
val_dir = "val"
test_dir = "test"

# Remove existing folders (if any)
shutil.rmtree(train_dir, ignore_errors=True)
shutil.rmtree(val_dir, ignore_errors=True)
shutil.rmtree(test_dir, ignore_errors=True)

# Split dataset (80% train, 10% validation, 10% test)
for category in os.listdir(dataset_path):
    img_path = os.path.join(dataset_path, category)
    images = os.listdir(img_path)
    np.random.shuffle(images)

    train_split = int(0.8 * len(images))
    val_split = int(0.9 * len(images))

    # Create folders
    os.makedirs(os.path.join(train_dir, category), exist_ok=True)
    os.makedirs(os.path.join(val_dir, category), exist_ok=True)
    os.makedirs(os.path.join(test_dir, category), exist_ok=True)

    # Move images
    for i, img in enumerate(images):
        src = os.path.join(img_path, img)
        if i < train_split:
            dest = os.path.join(train_dir, category, img)
        elif i < val_split:
            dest = os.path.join(val_dir, category, img)
        else:
            dest = os.path.join(test_dir, category, img)
        shutil.copy(src, dest)

# Image sizes
IMG_SIZE = 224
BATCH_SIZE = 32

# Data Augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Load images
train_generator = train_datagen.flow_from_directory(train_dir, target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')
val_generator = val_datagen.flow_from_directory(val_dir, target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')
test_generator = test_datagen.flow_from_directory(test_dir, target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')

num_classes = len(train_generator.class_indices)

def coatnet_block(x, filters):
    x = Conv2D(filters, (3, 3), padding="same", activation="relu")(x)
    x = BatchNormalization()(x)
    x = Conv2D(filters, (3, 3), padding="same", activation="relu")(x)
    x = BatchNormalization()(x)
    x = MaxPooling2D((2, 2))(x)
    return x

def build_coatnet(input_shape, num_classes):
    inputs = Input(shape=input_shape)

    # CoatNet-like structure: CNN + Transformer Hybrid
    x = coatnet_block(inputs, 64)
    x = coatnet_block(x, 128)
    x = coatnet_block(x, 256)

    # Global Average Pooling & Fully Connected Layers
    x = GlobalAveragePooling2D()(x)
    x = Dense(512, activation="relu")(x)
    x = Dropout(0.5)(x)
    x = Dense(num_classes, activation="softmax")(x)

    model = Model(inputs, x)
    return model

# Build the CoatNet model
model = build_coatnet((IMG_SIZE, IMG_SIZE, 3), num_classes)

# Compile model
model.compile(optimizer=Adam(learning_rate=0.0001), loss="categorical_crossentropy", metrics=["accuracy"])

# Model summary
model.summary()

EPOCHS = 10

start_time = time.time()
history = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS)
end_time = time.time()

# Training Time
training_time = end_time - start_time

# Evaluate test accuracy
test_loss, test_accuracy = model.evaluate(test_generator)

# Plot accuracy/loss curves
plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

# Calculate Model Size
model_size = sum([np.prod(v.shape) for v in model.trainable_variables])
model_size_MB = model_size * 4 / (1024 ** 2)  # Convert to MB

# Print Summary
print(f"Training Accuracy: {history.history['accuracy'][-1] * 100:.2f}%")
print(f"Validation Accuracy: {history.history['val_accuracy'][-1] * 100:.2f}%")
print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"Training Time: {training_time:.2f} seconds")
print(f"Model Size: {model_size_MB:.2f} MB")

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score, auc
from sklearn.preprocessing import label_binarize
import seaborn as sns
import itertools

# Generate predictions and true labels
y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = test_generator.classes
class_labels = list(test_generator.class_indices.keys())

# One-hot encode the true labels for multi-class AUC calculation
y_true_binarized = label_binarize(y_true, classes=range(len(class_labels)))

# 1️⃣ Precision, Recall, F1-Score
print("Classification Report:")
print(classification_report(y_true, y_pred_classes, target_names=class_labels))

# 2️⃣ Enhanced Confusion Matrix Visualization with Heatmap
def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap='Blues'):
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap=cmap, xticklabels=classes, yticklabels=classes)
    plt.title(title)
    plt.xlabel('Predicted label')
    plt.ylabel('True label')
    plt.show()

cm = confusion_matrix(y_true, y_pred_classes)
plot_confusion_matrix(cm, class_labels, title='Confusion Matrix')

# 3️⃣ AUC-ROC Curves for each class (One-vs-Rest strategy)
plt.figure(figsize=(12, 8))
for i, label in enumerate(class_labels):
    fpr, tpr, _ = roc_curve(y_true_binarized[:, i], y_pred[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'{label} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('AUC-ROC Curves for Each Class')
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# 4️⃣ Macro and Weighted AUC using OvR strategy
print(f"Macro-average AUC (OvR): {roc_auc_score(y_true_binarized, y_pred, average='macro'):.4f}")
print(f"Weighted-average AUC (OvR): {roc_auc_score(y_true_binarized, y_pred, average='weighted'):.4f}")
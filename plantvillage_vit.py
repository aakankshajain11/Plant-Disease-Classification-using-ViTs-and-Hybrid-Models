# -*- coding: utf-8 -*-
"""plantvillage_ViT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gd3XigrIXChzkjClCgvihFxExmhqY8Ao
"""

# STEP 1: Upload Kaggle credentials
from google.colab import files
uploaded = files.upload()  # Upload your kaggle.json

import os, zipfile
json_filename = list(uploaded.keys())[0]
os.environ['KAGGLE_CONFIG_DIR'] = "/content"
print(f"‚úÖ Using Kaggle credential: {json_filename}")

# STEP 2: Download Plant Village Dataset
!kaggle datasets download -d emmarex/plantdisease
!unzip -qo plantdisease.zip -d PlantVillageData

# STEP 3: Reorganize data into train/val/test split
import shutil, random
base_dir = "PlantVillageData/PlantVillage"
output_dirs = ['train', 'val', 'test']
for dir_name in output_dirs:
    shutil.rmtree(dir_name, ignore_errors=True)

all_classes = os.listdir(base_dir)
for cls in all_classes:
    imgs = os.listdir(os.path.join(base_dir, cls))
    random.shuffle(imgs)
    n = len(imgs)
    train_n, val_n = int(0.8 * n), int(0.1 * n)

    for split, img_range in zip(output_dirs, [imgs[:train_n], imgs[train_n:train_n+val_n], imgs[train_n+val_n:]]):
        cls_dir = os.path.join(split, cls)
        os.makedirs(cls_dir, exist_ok=True)
        for img in img_range:
            shutil.copy(os.path.join(base_dir, cls, img), os.path.join(cls_dir, img))

# STEP 4: Data generators
from tensorflow.keras.preprocessing.image import ImageDataGenerator

IMG_SIZE = 224
BATCH_SIZE = 32

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=25,
    zoom_range=0.15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    shear_range=0.1,
    fill_mode='nearest'
)
val_test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory('train', target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')
val_generator = val_test_datagen.flow_from_directory('val', target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')
test_generator = val_test_datagen.flow_from_directory('test', target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')

num_classes = len(train_generator.class_indices)
class_labels = list(train_generator.class_indices.keys())

# STEP 5: Define ViT Model
!pip install -q transformers
from transformers import TFAutoModel
import tensorflow as tf
from tensorflow.keras import layers, Model

class ViTLayer(layers.Layer):
    def __init__(self):
        super(ViTLayer, self).__init__()
        self.vit = TFAutoModel.from_pretrained('google/vit-base-patch16-224')
        self.vit.trainable = False

    def call(self, inputs):
        inputs = tf.transpose(inputs, [0, 3, 1, 2])
        return self.vit(pixel_values=inputs).last_hidden_state

def create_vit_model(num_classes):
    inputs = layers.Input(shape=(224, 224, 3))
    vit_outputs = ViTLayer()(inputs)
    pooled = layers.GlobalAveragePooling1D()(vit_outputs)
    x = layers.Dense(512, activation='relu')(pooled)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)
    return Model(inputs, outputs)

model = create_vit_model(num_classes)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# STEP 6: Train Model
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import time

early_stop = EarlyStopping(patience=5, restore_best_weights=True)
lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=3)

start_time = time.time()
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=10,
    callbacks=[early_stop, lr_scheduler]
)
training_time = time.time() - start_time

# STEP 7: Evaluation
import matplotlib.pyplot as plt
test_loss, test_accuracy = model.evaluate(test_generator)

plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Val Accuracy")
plt.title("Accuracy over Epochs")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.title("Loss over Epochs")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# STEP 8: Metrics Summary
import numpy as np
model_size = np.sum([np.prod(v.shape) for v in model.trainable_variables])
model_size_MB = model_size * 4 / (1024 ** 2)

print(f"\n‚úÖ Training Accuracy: {history.history['accuracy'][-1] * 100:.2f}%")
print(f"‚úÖ Validation Accuracy: {history.history['val_accuracy'][-1] * 100:.2f}%")
print(f"‚úÖ Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"‚è±Ô∏è Training Time: {training_time:.2f} seconds")
print(f"üì¶ Model Size: {model_size_MB:.2f} MB")

# STEP 9: Classification Report & Confusion Matrix
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import seaborn as sns

y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = test_generator.classes

print("\nüìä Classification Report:")
print(classification_report(y_true, y_pred_classes, target_names=class_labels))

cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# STEP 10: AUC-ROC
y_true_bin = label_binarize(y_true, classes=range(num_classes))
plt.figure(figsize=(12, 8))
for i, label in enumerate(class_labels):
    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f"{label} (AUC = {roc_auc:.2f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.title("AUC-ROC Curve (One-vs-Rest)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

print(f"üßÆ Macro-average AUC: {roc_auc_score(y_true_bin, y_pred, average='macro'):.4f}")
print(f"üßÆ Weighted-average AUC: {roc_auc_score(y_true_bin, y_pred, average='weighted'):.4f}")


# -*- coding: utf-8 -*-
"""GAN-iteration1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VGAYviiznulxP9HONMDzN2n3BLBNxHf4
"""

# Install and import required packages
import os
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Reshape, Flatten, Dropout, Input
from tensorflow.keras.layers import BatchNormalization, LeakyReLU, Conv2DTranspose, Conv2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from tensorflow.keras.initializers import RandomNormal
from tensorflow.keras.utils import plot_model
from zipfile import ZipFile
from google.colab import files
import tensorflow as tf
import time

# Upload the zipped dataset containing 'healthy' images
uploaded = files.upload()
# Extract the uploaded zip file
dataset_folder = "healthy_dataset"

for file in uploaded.keys():
    if file.endswith(".zip"):
        with ZipFile(file, 'r') as zip_ref:
            zip_ref.extractall(dataset_folder)

# Recursive image loader function
def load_images_recursive(root_folder, img_size=(64, 64)):
    image_list = []
    for root, dirs, files in os.walk(root_folder):
        for file in files:
            if file.lower().endswith(('.png', '.jpg', '.jpeg')):
                try:
                    img = load_img(os.path.join(root, file), target_size=img_size)
                    img = img_to_array(img)
                    img = (img - 127.5) / 127.5  # Normalize to [-1, 1]
                    image_list.append(img)
                except Exception as e:
                    print(f"Error loading {file}: {e}")
    return np.array(image_list)

# Load images safely from subdirectories
image_data = load_images_recursive(dataset_folder)
print(f"‚úÖ Loaded {image_data.shape[0]} images of shape {image_data.shape[1:]}")

plt.figure(figsize=(10, 4))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow((image_data[i] + 1) / 2)  # Convert back to [0,1]
    plt.axis('off')
plt.suptitle("Sample Healthy Images")
plt.tight_layout()
plt.show()

IMG_HEIGHT, IMG_WIDTH, CHANNELS = 64, 64, 3

def build_generator():
    model = Sequential()
    model.add(Dense(256 * 8 * 8, input_dim=100))
    model.add(LeakyReLU(0.2))
    model.add(Reshape((8, 8, 256)))
    model.add(BatchNormalization())

    model.add(Conv2DTranspose(128, 4, strides=2, padding='same'))
    model.add(LeakyReLU(0.2))
    model.add(BatchNormalization())

    model.add(Conv2DTranspose(64, 4, strides=2, padding='same'))
    model.add(LeakyReLU(0.2))
    model.add(BatchNormalization())

    model.add(Conv2DTranspose(CHANNELS, 4, strides=2, padding='same', activation='tanh'))
    return model

def build_discriminator():
    model = Sequential()
    model.add(Conv2D(64, 4, strides=2, padding='same', input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS)))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.3))

    model.add(Conv2D(128, 4, strides=2, padding='same'))
    model.add(LeakyReLU(0.2))
    model.add(Dropout(0.3))

    model.add(Flatten())
    model.add(Dense(1, activation='sigmoid'))
    return model

generator = build_generator()
discriminator = build_discriminator()

# Compile discriminator
discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])

# GAN model
discriminator.trainable = False
gan_input = Input(shape=(100,))
gen_output = generator(gan_input)
validity = discriminator(gen_output)

gan = Model(gan_input, validity)
gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))

EPOCHS = 3000
BATCH_SIZE = 32
SAVE_INTERVAL = 500
generated_images_collection = []

def generate_and_plot_images(epoch):
    noise = np.random.normal(0, 1, (10, 100))
    gen_imgs = generator.predict(noise)
    gen_imgs = 0.5 * gen_imgs + 0.5

    plt.figure(figsize=(10, 4))
    for i in range(10):
        plt.subplot(2, 5, i+1)
        plt.imshow(gen_imgs[i])
        plt.axis('off')
    plt.suptitle(f"Epoch {epoch}: Synthetic Images")
    plt.tight_layout()
    plt.show()

    generated_images_collection.extend(gen_imgs)

def train_gan(X_train, epochs, batch_size):
    half_batch = batch_size // 2
    for epoch in range(1, epochs + 1):
        idx = np.random.randint(0, X_train.shape[0], half_batch)
        real_imgs = X_train[idx]

        noise = np.random.normal(0, 1, (half_batch, 100))
        fake_imgs = generator.predict(noise)

        d_loss_real = discriminator.train_on_batch(real_imgs, np.ones((half_batch, 1)))
        d_loss_fake = discriminator.train_on_batch(fake_imgs, np.zeros((half_batch, 1)))
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        noise = np.random.normal(0, 1, (batch_size, 100))
        g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))

        if epoch % 100 == 0:
            print(f"{epoch} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]")
        if epoch % SAVE_INTERVAL == 0:
            generate_and_plot_images(epoch)

start = time.time()
train_gan(image_data, EPOCHS, BATCH_SIZE)
print("‚úÖ Training completed in {:.2f} minutes".format((time.time() - start)/60))

def generate_final_images(n_images=50):
    noise = np.random.normal(0, 1, (n_images, 100))
    gen_imgs = generator.predict(noise)
    gen_imgs = 0.5 * gen_imgs + 0.5
    return gen_imgs

final_images = generate_final_images(50)

# Display 10 generated images
plt.figure(figsize=(10, 5))
for i in range(10):
    plt.subplot(2, 5, i + 1)
    plt.imshow(final_images[i])
    plt.axis('off')
plt.suptitle("Final Synthetic Images")
plt.tight_layout()
plt.show()

import cv2

save_dir = "/content/generated_synthetic_images"
os.makedirs(save_dir, exist_ok=True)

for idx, img in enumerate(final_images):
    img_uint8 = (img * 255).astype(np.uint8)
    cv2.imwrite(f"{save_dir}/synthetic_{idx+1}.png", cv2.cvtColor(img_uint8, cv2.COLOR_RGB2BGR))

print("üñºÔ∏è 50 Synthetic Images saved in:", save_dir)
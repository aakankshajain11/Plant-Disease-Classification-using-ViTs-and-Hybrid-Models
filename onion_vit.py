# -*- coding: utf-8 -*-
"""onion_ViT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Yf7uiV4DcYvloxVmNret6rbMJVFLGj7
"""

# STEP 1: Upload GCP credentials
from google.colab import files
uploaded = files.upload()  # Upload your onion11-key.json

# STEP 2: Set up environment and download from GCS
import os
json_filename = list(uploaded.keys())[0]
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = json_filename
print(f"‚úÖ Using GCP service account: {json_filename}")

GCS_URI = "gs://onion11"
!mkdir -p OnionData
!gsutil -m cp -r {GCS_URI}/* OnionData/

# STEP 3: Data preprocessing (same as CoatNet)
import shutil, random, warnings
from tensorflow.keras.preprocessing.image import ImageDataGenerator

base_dir = 'OnionData'
class_id_to_name = {}
classes_txt_path = os.path.join(base_dir, 'classes.txt')
if os.path.exists(classes_txt_path):
    with open(classes_txt_path, 'r') as f:
        for i, line in enumerate(f):
            class_id_to_name[str(i)] = line.strip()

jpgs = {os.path.splitext(f)[0]: f for f in os.listdir(base_dir) if f.endswith('.jpg')}
txts = {os.path.splitext(f)[0]: f for f in os.listdir(base_dir) if f.endswith('.txt')}
paired = [base for base in jpgs if base in txts]

class_to_images = {}
for base in paired:
    txt_path = os.path.join(base_dir, txts[base])
    try:
        with open(txt_path, 'r') as f:
            line = f.readline().strip()
            if not line:
                continue
            class_id = line.split()[0]
            class_label = class_id_to_name.get(class_id, f"class_{class_id}")
            class_to_images.setdefault(class_label, []).append(jpgs[base])
    except Exception as e:
        print(f"Skipping {txt_path} due to error: {e}")

for split in ['train', 'val', 'test']:
    shutil.rmtree(split, ignore_errors=True)
    for label in class_to_images:
        os.makedirs(os.path.join(split, label), exist_ok=True)

for label, images in class_to_images.items():
    random.shuffle(images)
    total = len(images)
    train_n = int(0.8 * total)
    val_n = int(0.1 * total)
    test_n = total - train_n - val_n
    for img in images[:train_n]:
        shutil.copy(os.path.join(base_dir, img), os.path.join('train', label, img))
    for img in images[train_n:train_n+val_n]:
        shutil.copy(os.path.join(base_dir, img), os.path.join('val', label, img))
    for img in images[train_n+val_n:]:
        shutil.copy(os.path.join(base_dir, img), os.path.join('test', label, img))

# 4. Data generators with augmentation
from tensorflow.keras.preprocessing.image import ImageDataGenerator

IMG_SIZE = 224
BATCH_SIZE = 32

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=25,
    zoom_range=0.15,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True,
    shear_range=0.1,
    fill_mode='nearest'
)

val_test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory('train', target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')
val_generator = val_test_datagen.flow_from_directory('val', target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')
test_generator = val_test_datagen.flow_from_directory('test', target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')

num_classes = len(train_generator.class_indices)
class_labels = list(train_generator.class_indices.keys())

# 5. Define ViT model with frozen base
!pip install -q transformers
from transformers import TFAutoModel
import tensorflow as tf
from tensorflow.keras import layers, Model

class ViTLayer(layers.Layer):
    def __init__(self):
        super(ViTLayer, self).__init__()
        self.vit = TFAutoModel.from_pretrained('google/vit-base-patch16-224')
        self.vit.trainable = False  # Freeze pretrained layers

    def call(self, inputs):
        inputs = tf.transpose(inputs, [0, 3, 1, 2])
        return self.vit(pixel_values=inputs).last_hidden_state

def create_vit_model(num_classes):
    inputs = layers.Input(shape=(224, 224, 3))
    vit_outputs = ViTLayer()(inputs)
    pooled = layers.GlobalAveragePooling1D()(vit_outputs)
    x = layers.Dense(512, activation='relu')(pooled)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)
    return Model(inputs, outputs)

model = create_vit_model(num_classes)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# 6. Train model with EarlyStopping and ReduceLROnPlateau
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

early_stop = EarlyStopping(patience=5, restore_best_weights=True)
lr_scheduler = ReduceLROnPlateau(factor=0.5, patience=3)

import time
start_time = time.time()
history = model.fit(
    train_generator,
    validation_data=val_generator,
    epochs=10,
    callbacks=[early_stop, lr_scheduler]
)
training_time = time.time() - start_time

# STEP 7: Evaluate Model
import matplotlib.pyplot as plt
test_loss, test_accuracy = model.evaluate(test_generator)

# Curves
plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Val Accuracy")
plt.title("Accuracy over Epochs")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.grid(True)
plt.show()

plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.title("Loss over Epochs")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

# STEP 8: Metrics Summary
import numpy as np
model_size = np.sum([np.prod(v.shape) for v in model.trainable_variables])
model_size_MB = model_size * 4 / (1024 ** 2)

print(f"\n‚úÖ Training Accuracy: {history.history['accuracy'][-1] * 100:.2f}%")
print(f"‚úÖ Validation Accuracy: {history.history['val_accuracy'][-1] * 100:.2f}%")
print(f"‚úÖ Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"‚è±Ô∏è Training Time: {training_time:.2f} seconds")
print(f"üì¶ Model Size: {model_size_MB:.2f} MB")

# STEP 9: Classification Report & Confusion Matrix
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import seaborn as sns

y_pred = model.predict(test_generator)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = test_generator.classes

print("\nüìä Classification Report:")
print(classification_report(y_true, y_pred_classes, target_names=class_labels))

cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# STEP 10: AUC-ROC Curve
y_true_bin = label_binarize(y_true, classes=range(num_classes))
plt.figure(figsize=(12, 8))
for i, label in enumerate(class_labels):
    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f"{label} (AUC = {roc_auc:.2f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.title("AUC-ROC Curve (One-vs-Rest)")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

# Macro AUC
print(f"üßÆ Macro-average AUC: {roc_auc_score(y_true_bin, y_pred, average='macro'):.4f}")
print(f"üßÆ Weighted-average AUC: {roc_auc_score(y_true_bin, y_pred, average='weighted'):.4f}")
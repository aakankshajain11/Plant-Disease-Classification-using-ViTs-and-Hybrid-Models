# -*- coding: utf-8 -*-
"""DenseNet121.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ih7I3Q_f4mEk2QMkB1gowAVSkiuk_YZQ
"""

# Import necessary libraries
import os
import zipfile
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torchvision import transforms, datasets
from sklearn.model_selection import train_test_split
from tqdm.notebook import tqdm
import time

# STEP 1: Upload Kaggle API token
from google.colab import files
uploaded = files.upload()

# Ensure Kaggle API key is in place
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Download the dataset
!kaggle datasets download -d emmarex/plantdisease

# Unzip the downloaded dataset
!unzip plantdisease.zip -d plant_disease

# STEP 3: Organize dataset into train/val/test folders (Corrected)
import shutil
import random
import os

# Define source and output paths
data_dir = 'plant_disease/PlantVillage'
output_dirs = ['train', 'val', 'test']
valid_exts = ('.jpg', '.jpeg', '.png')

# Clear previous folders
for d in output_dirs:
    shutil.rmtree(d, ignore_errors=True)

# Loop through all folders (classes)
classes = [cls for cls in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, cls))]

for cls in classes:
    img_folder = os.path.join(data_dir, cls)
    images = [f for f in os.listdir(img_folder) if f.lower().endswith(valid_exts)]

    if len(images) < 10:
        continue  # Skip classes with too few samples

    random.shuffle(images)
    n = len(images)
    train_n = int(0.8 * n)
    val_n = int(0.1 * n)

    for i, img in enumerate(images):
        split = 'train' if i < train_n else 'val' if i < train_n + val_n else 'test'
        dst_dir = os.path.join(split, cls)
        os.makedirs(dst_dir, exist_ok=True)
        shutil.copy(os.path.join(img_folder, img), os.path.join(dst_dir, img))

# STEP 4: Data loading
import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

IMG_SIZE = 224
BATCH_SIZE = 32

train_gen = ImageDataGenerator(rescale=1./255)
val_gen = ImageDataGenerator(rescale=1./255)
test_gen = ImageDataGenerator(rescale=1./255)

train_loader = train_gen.flow_from_directory('train', target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')
val_loader = val_gen.flow_from_directory('val', target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')
test_loader = test_gen.flow_from_directory('test', target_size=(IMG_SIZE, IMG_SIZE), batch_size=BATCH_SIZE, class_mode='categorical')

print("Detected classes:", train_loader.class_indices)

# STEP 5: Build DenseNet121 model
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.models import Model

base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(512, activation='relu')(x)
x = Dropout(0.5)(x)
predictions = Dense(len(train_loader.class_indices), activation='softmax')(x)
model = Model(inputs=base_model.input, outputs=predictions)

for layer in base_model.layers:
    layer.trainable = False

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# STEP 6: Train the model
import time
EPOCHS = 10
start_time = time.time()

history = model.fit(train_loader, validation_data=val_loader, epochs=EPOCHS)
training_time = time.time() - start_time

# STEP 7: Evaluate model
import matplotlib.pyplot as plt

test_loss, test_accuracy = model.evaluate(test_loader)

plt.plot(history.history["accuracy"], label="Train Accuracy")
plt.plot(history.history["val_accuracy"], label="Validation Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Accuracy over Epochs")
plt.grid(True)
plt.show()

# STEP 8: Classification report, Confusion matrix, AUC
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, auc
from sklearn.preprocessing import label_binarize
import seaborn as sns

y_pred = model.predict(test_loader)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = test_loader.classes
class_labels = list(test_loader.class_indices.keys())

print("\nðŸ“Š Classification Report:")
print(classification_report(y_true, y_pred_classes, target_names=class_labels))

cm = confusion_matrix(y_true, y_pred_classes)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

# AUC-ROC
y_true_bin = label_binarize(y_true, classes=range(len(class_labels)))

plt.figure(figsize=(12, 8))
for i, label in enumerate(class_labels):
    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f"{label} (AUC = {roc_auc:.2f})")

plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("AUC-ROC Curve (One-vs-Rest)")
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

# STEP 9: Final metrics summary
model_size_MB = sum([np.prod(v.shape) for v in model.trainable_variables]) * 4 / (1024 ** 2)

print(f"\nâœ… Training Accuracy: {history.history['accuracy'][-1] * 100:.2f}%")
print(f"âœ… Validation Accuracy: {history.history['val_accuracy'][-1] * 100:.2f}%")
print(f"âœ… Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"â±ï¸ Training Time: {training_time:.2f} seconds")
print(f"ðŸ“¦ Model Size: {model_size_MB:.2f} MB")
print(f"ðŸ§® Macro-average AUC: {roc_auc_score(y_true_bin, y_pred, average='macro'):.4f}")
print(f"ðŸ§® Weighted-average AUC: {roc_auc_score(y_true_bin, y_pred, average='weighted'):.4f}")